{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa13b799",
   "metadata": {},
   "source": [
    "\n",
    "# Implementación manual — A2C, PPO y SAC sobre *PointEnv*\n",
    "\n",
    "Este notebook consolida **tus tres scripts originales** en un único flujo ejecutable, con breves explicaciones:\n",
    "\n",
    "- **A2C** (entorno discreto)  \n",
    "- **PPO** (entorno discreto)  \n",
    "- **SAC** (entorno continuo)  \n",
    "\n",
    "Se preserva la lógica original de tus archivos `.py` y se **mantienen las salidas** (gráficos y PDFs) en las carpetas correspondientes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f260e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configuración básica para notebooks\n",
    "%matplotlib inline\n",
    "import os\n",
    "print(\"CWD:\", os.getcwd())\n",
    "\n",
    "# Si querés redirigir las salidas, podés exportar variables de entorno aquí.\n",
    "# Por defecto, cada script ya crea su propia carpeta ./output_* si no existe.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911855ca",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## 1) A2C — *Advantage Actor–Critic* (acciones discretas)\n",
    "\n",
    "**Idea:** política categórica para elegir entre {-1, 0, +1}, crítico para estimar el valor,  \n",
    "y entrenamiento por ventajas \\( \\hat{A} = G_t - V(s_t) \\) con **entropía** para explorar.\n",
    "\n",
    "**Ejecución:** al correr la celda siguiente, entrena por 400 episodios, guarda el modelo y genera:\n",
    "- `./output_a2c_pointenv/a2c_pointenv_model.pth`\n",
    "- `./output_a2c_pointenv/convergence_a2c.png`\n",
    "- `./output_a2c_pointenv/A2C_PointEnv_Report.pdf`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd440f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "a2c_pointenv.py\n",
    "A2C (Advantage Actor-Critic) en PointEnv discreto (acciones {-1,0,+1}).\n",
    "Guarda: modelo, convergence.png, A2C_PointEnv_Report.pdf, README.txt\n",
    "Requerimientos: torch, matplotlib, numpy\n",
    "Ejecutar: python a2c_pointenv.py\n",
    "\"\"\"\n",
    "import os, math, random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from datetime import datetime\n",
    "\n",
    "# ----------------------------\n",
    "# Output dir (portable)\n",
    "OUT_DIR = os.environ.get(\"OUT_A2C\", \"./output_a2c_pointenv\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# ----------------------------\n",
    "# Entorno discreto simple\n",
    "class PointEnvDiscrete:\n",
    "    def __init__(self, max_steps=50, force=0.6, noise_scale=0.02):\n",
    "        self.max_steps = max_steps\n",
    "        self.force = force\n",
    "        self.noise_scale = noise_scale\n",
    "        self.reset()\n",
    "    def reset(self):\n",
    "        self.pos = np.random.uniform(-1.5, 1.5)\n",
    "        self.steps = 0\n",
    "        self.target = 0.0\n",
    "        return np.array([self.pos], dtype=np.float32)\n",
    "    def step(self, action_index):\n",
    "        act = [-1, 0, 1][action_index]\n",
    "        noise = np.random.normal(scale=self.noise_scale)\n",
    "        self.pos = self.pos + act * self.force + noise\n",
    "        self.steps += 1\n",
    "        dist = abs(self.pos - self.target)\n",
    "        reward = -dist\n",
    "        done = False\n",
    "        if dist < 0.05:\n",
    "            reward += 1.0\n",
    "            done = True\n",
    "        if self.steps >= self.max_steps:\n",
    "            done = True\n",
    "        return np.array([self.pos], dtype=np.float32), float(reward), done, {}\n",
    "    @property\n",
    "    def obs_shape(self):\n",
    "        return (1,)\n",
    "    @property\n",
    "    def n_actions(self):\n",
    "        return 3\n",
    "\n",
    "# ----------------------------\n",
    "# Actor-Critic network\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden=64):\n",
    "        super().__init__()\n",
    "        self.shared = nn.Sequential(nn.Linear(obs_dim, hidden), nn.Tanh())\n",
    "        self.policy = nn.Sequential(nn.Linear(hidden, hidden//2), nn.Tanh(), nn.Linear(hidden//2, act_dim))\n",
    "        self.value  = nn.Sequential(nn.Linear(hidden, hidden//2), nn.Tanh(), nn.Linear(hidden//2, 1))\n",
    "    def forward(self, x):\n",
    "        h = self.shared(x)\n",
    "        logits = self.policy(h)\n",
    "        value = self.value(h).squeeze(-1)\n",
    "        return logits, value\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers\n",
    "def select_action(logits):\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    dist = torch.distributions.Categorical(probs)\n",
    "    a = dist.sample()\n",
    "    return a.item(), dist.log_prob(a), dist.entropy()\n",
    "\n",
    "def discounted_returns(rewards, dones, last_value, gamma=0.99):\n",
    "    R = last_value\n",
    "    returns = []\n",
    "    for r, d in zip(rewards[::-1], dones[::-1]):\n",
    "        if d:\n",
    "            R = 0.0\n",
    "        R = r + gamma * R\n",
    "        returns.insert(0, R)\n",
    "    return returns\n",
    "\n",
    "# ----------------------------\n",
    "# Hiperparámetros\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "NUM_EPISODES = 400\n",
    "MAX_STEPS = 50\n",
    "GAMMA = 0.99\n",
    "LR = 2.5e-4\n",
    "VALUE_COEF = 0.5\n",
    "ENTROPY_COEF = 1e-3\n",
    "PRINT_EVERY = 25\n",
    "\n",
    "# ----------------------------\n",
    "# Setup\n",
    "env = PointEnvDiscrete(max_steps=MAX_STEPS)\n",
    "obs_dim = env.obs_shape[0]\n",
    "act_dim = env.n_actions\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ActorCritic(obs_dim, act_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "episode_rewards = []\n",
    "smoothed = []\n",
    "alpha_smooth = 0.04\n",
    "\n",
    "# Training loop\n",
    "for ep in range(1, NUM_EPISODES + 1):\n",
    "    obs = env.reset()\n",
    "    obs_t = torch.tensor(obs, dtype=torch.float32, device=device)\n",
    "    done = False\n",
    "    ep_reward = 0.0\n",
    "\n",
    "    # buffers\n",
    "    logps = []\n",
    "    values = []\n",
    "    rewards = []\n",
    "    dones = []\n",
    "    entropies = []\n",
    "\n",
    "    for step in range(MAX_STEPS):\n",
    "        logits, value = model(obs_t.unsqueeze(0))\n",
    "        logits = logits.squeeze(0)\n",
    "        value = value.squeeze(0)\n",
    "        action, logp, entropy = select_action(logits)\n",
    "        next_obs, reward, done, _ = env.step(action)\n",
    "        ep_reward += reward\n",
    "\n",
    "        logps.append(logp)\n",
    "        values.append(value)\n",
    "        rewards.append(reward)\n",
    "        dones.append(done)\n",
    "        entropies.append(entropy)\n",
    "\n",
    "        obs_t = torch.tensor(next_obs, dtype=torch.float32, device=device)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    if done:\n",
    "        last_value = 0.0\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            _, last_value = model(obs_t.unsqueeze(0))\n",
    "            last_value = last_value.item()\n",
    "\n",
    "    returns = discounted_returns(rewards, dones, last_value, GAMMA)\n",
    "    returns = torch.tensor(returns, dtype=torch.float32, device=device)\n",
    "    values = torch.stack(values)\n",
    "    logps = torch.stack(logps)\n",
    "    entropies = torch.stack(entropies)\n",
    "    advantages = returns - values.detach()\n",
    "\n",
    "    value_loss = (advantages ** 2).mean()\n",
    "    policy_loss = -(logps * advantages).mean()\n",
    "    entropy_loss = -entropies.mean()\n",
    "\n",
    "    loss = policy_loss + VALUE_COEF * value_loss + ENTROPY_COEF * entropy_loss\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # optional: torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "    optimizer.step()\n",
    "\n",
    "    episode_rewards.append(ep_reward)\n",
    "    if len(smoothed) == 0:\n",
    "        smoothed.append(ep_reward)\n",
    "    else:\n",
    "        smoothed.append(smoothed[-1] * (1 - alpha_smooth) + ep_reward * alpha_smooth)\n",
    "\n",
    "    if (ep % PRINT_EVERY == 0) or (ep == 1):\n",
    "        avg100 = np.mean(episode_rewards[-100:]) if len(episode_rewards) >= 1 else 0.0\n",
    "        print(f\"Ep {ep}/{NUM_EPISODES} Reward {ep_reward:.3f} Avg100 {avg100:.3f} Loss {loss.item():.4f}\")\n",
    "\n",
    "# Save model and results\n",
    "model_path = os.path.join(OUT_DIR, \"a2c_pointenv_model.pth\")\n",
    "torch.save(model.state_dict(), model_path)\n",
    "\n",
    "# Plot\n",
    "episodes = np.arange(1, len(episode_rewards) + 1)\n",
    "plt.figure(figsize=(9,5))\n",
    "plt.plot(episodes, episode_rewards, alpha=0.25, label=\"Reward por episodio\")\n",
    "plt.plot(episodes, smoothed, label=f\"Media exponencial α={alpha_smooth}\")\n",
    "plt.xlabel(\"Episodio\"); plt.ylabel(\"Reward\"); plt.title(\"A2C Convergencia - PointEnv (discreto)\")\n",
    "plt.legend(); plt.grid(True)\n",
    "plot_path = os.path.join(OUT_DIR, \"convergence_a2c.png\")\n",
    "plt.savefig(plot_path, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "# PDF report (3 páginas)\n",
    "pdf_path = os.path.join(OUT_DIR, \"A2C_PointEnv_Report.pdf\")\n",
    "with PdfPages(pdf_path) as pdf:\n",
    "    # Page 1 - description\n",
    "    plt.figure(figsize=(8.27, 11.69)); plt.axis(\"off\")\n",
    "    txt = [\n",
    "        \"A2C aplicado a PointEnv (discreto)\",\n",
    "        \"\",\n",
    "        f\"Fecha: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\",\n",
    "        \"\",\n",
    "        \"Problema: punto 1D que debe llegar a 0.0. Acciones discretas {-1,0,+1}.\",\n",
    "        \"Algoritmo: A2C (Actor-Critic).\",\n",
    "        \"\",\n",
    "        f\"Episodes: {NUM_EPISODES}, max_steps: {MAX_STEPS}\",\n",
    "        \"\",\n",
    "        \"Se incluyen fragmentos de código y gráfica de convergencia.\"\n",
    "    ]\n",
    "    plt.text(0.02, 0.98, \"\\n\".join(txt), va=\"top\", wrap=True, fontsize=11)\n",
    "    pdf.savefig(); plt.close()\n",
    "\n",
    "    # Page 2 - code snippets\n",
    "    plt.figure(figsize=(8.27, 11.69)); plt.axis(\"off\")\n",
    "    code = [\n",
    "        \"Fragmentos de código:\",\n",
    "        \"\",\n",
    "        \"ActorCritic: shared -> policy logits ; value scalar\",\n",
    "        \"select_action: Categorical(softmax(logits))\",\n",
    "        \"returns = discounted_rewards; advantages = returns - values.detach()\",\n",
    "        \"loss = policy_loss + VALUE_COEF * value_loss + ENTROPY_COEF * entropy_loss\"\n",
    "    ]\n",
    "    plt.text(0.02, 0.98, \"\\n\".join(code), va=\"top\", wrap=True, fontsize=10)\n",
    "    pdf.savefig(); plt.close()\n",
    "\n",
    "    # Page 3 - plot\n",
    "    img = plt.imread(plot_path)\n",
    "    plt.figure(figsize=(8.27, 11.69)); plt.imshow(img); plt.axis(\"off\")\n",
    "    pdf.savefig(); plt.close()\n",
    "\n",
    "# README\n",
    "readme_path = os.path.join(OUT_DIR, \"README.txt\")\n",
    "with open(readme_path, \"w\") as f:\n",
    "    f.write(\"A2C PointEnv - Output\\n\")\n",
    "    f.write(f\"Model: {model_path}\\nPlot: {plot_path}\\nPDF: {pdf_path}\\nEpisodes: {NUM_EPISODES}\\n\")\n",
    "print(\"A2C: entrenamiento finalizado. Archivos en:\", OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036a4684",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## 2) PPO — *Proximal Policy Optimization* (discreto)\n",
    "\n",
    "**Clipped surrogate objective**: maximiza un bound del objetivo por razones de estabilidad;  \n",
    "usa **GAE(\\(\\lambda\\))** y múltiples épocas sobre el mismo batch de trayectoria.\n",
    "\n",
    "**Ejecución:** al correr la celda siguiente, entrena por 400 episodios, guarda y produce:\n",
    "- `./output_ppo_pointenv/ppo_pointenv_model.pth`\n",
    "- `./output_ppo_pointenv/convergence_ppo.png`\n",
    "- `./output_ppo_pointenv/PPO_PointEnv_Report.pdf`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb187f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "ppo_pointenv.py\n",
    "PPO (Clipped surrogate) en PointEnv discreto.\n",
    "Guarda: modelo, convergence.png, PPO_PointEnv_Report.pdf, README.txt\n",
    "Ejecutar: python ppo_pointenv.py\n",
    "\"\"\"\n",
    "import os, random\n",
    "import numpy as np\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from datetime import datetime\n",
    "\n",
    "OUT_DIR = os.environ.get(\"OUT_PPO\", \"./output_ppo_pointenv\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Same discrete environment as A2C\n",
    "class PointEnvDiscrete:\n",
    "    def __init__(self, max_steps=50, force=0.6, noise_scale=0.02):\n",
    "        self.max_steps = max_steps\n",
    "        self.force = force\n",
    "        self.noise_scale = noise_scale\n",
    "        self.reset()\n",
    "    def reset(self):\n",
    "        self.pos = np.random.uniform(-1.5, 1.5)\n",
    "        self.steps = 0\n",
    "        self.target = 0.0\n",
    "        return np.array([self.pos], dtype=np.float32)\n",
    "    def step(self, action_index):\n",
    "        act = [-1, 0, 1][action_index]\n",
    "        noise = np.random.normal(scale=self.noise_scale)\n",
    "        self.pos = self.pos + act * self.force + noise\n",
    "        self.steps += 1\n",
    "        dist = abs(self.pos - self.target)\n",
    "        reward = -dist\n",
    "        done = False\n",
    "        if dist < 0.05:\n",
    "            reward += 1.0\n",
    "            done = True\n",
    "        if self.steps >= self.max_steps:\n",
    "            done = True\n",
    "        return np.array([self.pos], dtype=np.float32), float(reward), done, {}\n",
    "    @property\n",
    "    def obs_shape(self):\n",
    "        return (1,)\n",
    "    @property\n",
    "    def n_actions(self):\n",
    "        return 3\n",
    "\n",
    "# Network: shared base, logits and value\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden=64):\n",
    "        super().__init__()\n",
    "        self.shared = nn.Sequential(nn.Linear(obs_dim, hidden), nn.Tanh())\n",
    "        self.policy = nn.Sequential(nn.Linear(hidden, hidden//2), nn.Tanh(), nn.Linear(hidden//2, act_dim))\n",
    "        self.value  = nn.Sequential(nn.Linear(hidden, hidden//2), nn.Tanh(), nn.Linear(hidden//2, 1))\n",
    "    def forward(self, x):\n",
    "        h = self.shared(x)\n",
    "        logits = self.policy(h)\n",
    "        v = self.value(h).squeeze(-1)\n",
    "        return logits, v\n",
    "\n",
    "def sample_action_and_logp(logits):\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    dist = torch.distributions.Categorical(probs)\n",
    "    a = dist.sample()\n",
    "    return a.item(), dist.log_prob(a), dist.entropy()\n",
    "\n",
    "def compute_gae(rewards, values, dones, last_value, gamma=0.99, lam=0.95):\n",
    "    values = list(values) + [last_value]\n",
    "    gae = 0.0\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        delta = rewards[step] + gamma * values[step+1] * (1 - dones[step]) - values[step]\n",
    "        gae = delta + gamma * lam * (1 - dones[step]) * gae\n",
    "        returns.insert(0, gae + values[step])\n",
    "    adv = np.array(returns) - np.array(values[:-1])\n",
    "    return returns, adv\n",
    "\n",
    "# Hiperparams\n",
    "SEED = 42; np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "NUM_EPOCHS = 400\n",
    "MAX_STEPS = 50\n",
    "GAMMA = 0.99; LAM = 0.95\n",
    "LR = 3e-4\n",
    "CLIP = 0.2\n",
    "UPDATE_EPOCHS = 6\n",
    "BATCH_SIZE = 64\n",
    "ENTROPY_COEF = 1e-3\n",
    "VALUE_COEF = 0.5\n",
    "\n",
    "# Setup\n",
    "env = PointEnvDiscrete(max_steps=MAX_STEPS)\n",
    "obs_dim = env.obs_shape[0]; act_dim = env.n_actions\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ActorCritic(obs_dim, act_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "episode_rewards = []\n",
    "smoothed = []; alpha = 0.04\n",
    "\n",
    "# Training loop - collect trajectories then update\n",
    "for ep in range(1, NUM_EPOCHS+1):\n",
    "    # collect one episode traj\n",
    "    obs = env.reset()\n",
    "    obs_t = torch.tensor(obs, dtype=torch.float32, device=device)\n",
    "    done = False\n",
    "    ep_reward = 0.0\n",
    "\n",
    "    obs_buf = []; actions_buf = []; logp_buf = []; rewards_buf = []; dones_buf = []; values_buf = []\n",
    "\n",
    "    for step in range(MAX_STEPS):\n",
    "        logits, value = model(obs_t.unsqueeze(0))\n",
    "        logits = logits.squeeze(0); value = value.squeeze(0)\n",
    "        a, logp, entropy = sample_action_and_logp(logits)\n",
    "        next_obs, reward, done, _ = env.step(a)\n",
    "        ep_reward += reward\n",
    "\n",
    "        obs_buf.append(obs_t.cpu().numpy())\n",
    "        actions_buf.append(a)\n",
    "        logp_buf.append(logp.item())\n",
    "        rewards_buf.append(reward)\n",
    "        dones_buf.append(done)\n",
    "        values_buf.append(value.item())\n",
    "\n",
    "        obs_t = torch.tensor(next_obs, dtype=torch.float32, device=device)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    if done:\n",
    "        last_value = 0.0\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            _, last_value = model(obs_t.unsqueeze(0))\n",
    "            last_value = last_value.item()\n",
    "\n",
    "    returns, advantages = compute_gae(rewards_buf, values_buf, dones_buf, last_value, GAMMA, LAM)\n",
    "    returns = torch.tensor(returns, dtype=torch.float32, device=device)\n",
    "    advantages = torch.tensor(advantages, dtype=torch.float32, device=device)\n",
    "    obs_arr = torch.tensor(np.vstack(obs_buf), dtype=torch.float32, device=device)\n",
    "    actions_arr = torch.tensor(actions_buf, dtype=torch.long, device=device)\n",
    "    old_logps = torch.tensor(logp_buf, dtype=torch.float32, device=device)\n",
    "\n",
    "    # PPO update - multiple epochs over the collected batch\n",
    "    dataset_size = len(actions_buf)\n",
    "    for _ in range(UPDATE_EPOCHS):\n",
    "        # simple mini-batching\n",
    "        idxs = np.arange(dataset_size)\n",
    "        np.random.shuffle(idxs)\n",
    "        for start in range(0, dataset_size, BATCH_SIZE):\n",
    "            end = start + BATCH_SIZE\n",
    "            mb_idx = idxs[start:end]\n",
    "            mb_obs = obs_arr[mb_idx]\n",
    "            mb_actions = actions_arr[mb_idx]\n",
    "            mb_oldlogp = old_logps[mb_idx]\n",
    "            mb_returns = returns[mb_idx]\n",
    "            mb_adv = advantages[mb_idx]\n",
    "\n",
    "            logits, vals = model(mb_obs)\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            dist = torch.distributions.Categorical(probs)\n",
    "            mb_logp = dist.log_prob(mb_actions)\n",
    "            ratio = torch.exp(mb_logp - mb_oldlogp)\n",
    "            surr1 = ratio * mb_adv\n",
    "            surr2 = torch.clamp(ratio, 1.0 - CLIP, 1.0 + CLIP) * mb_adv\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "            value_loss = (mb_returns - vals).pow(2).mean()\n",
    "            entropy_loss = -dist.entropy().mean()\n",
    "            loss = policy_loss + VALUE_COEF * value_loss + ENTROPY_COEF * entropy_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    episode_rewards.append(ep_reward)\n",
    "    if len(smoothed) == 0:\n",
    "        smoothed.append(ep_reward)\n",
    "    else:\n",
    "        smoothed.append(smoothed[-1] * (1 - alpha) + ep_reward * alpha)\n",
    "\n",
    "    if (ep % 25 == 0) or (ep == 1):\n",
    "        avg100 = np.mean(episode_rewards[-100:]) if len(episode_rewards) >= 1 else 0.0\n",
    "        print(f\"Ep {ep}/{NUM_EPOCHS} Reward {ep_reward:.3f} Avg100 {avg100:.3f}\")\n",
    "\n",
    "# Save\n",
    "model_path = os.path.join(OUT_DIR, \"ppo_pointenv_model.pth\")\n",
    "torch.save(model.state_dict(), model_path)\n",
    "# Plot\n",
    "episodes = np.arange(1, len(episode_rewards)+1)\n",
    "plt.figure(figsize=(9,5))\n",
    "plt.plot(episodes, episode_rewards, alpha=0.3, label=\"Reward por episodio\")\n",
    "plt.plot(episodes, smoothed, label=\"Media exponencial\")\n",
    "plt.xlabel(\"Episodio\"); plt.ylabel(\"Reward\"); plt.title(\"PPO Convergencia - PointEnv\")\n",
    "plt.legend(); plt.grid(True)\n",
    "plot_path = os.path.join(OUT_DIR, \"convergence_ppo.png\")\n",
    "plt.savefig(plot_path, bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "# PDF\n",
    "pdf_path = os.path.join(OUT_DIR, \"PPO_PointEnv_Report.pdf\")\n",
    "with PdfPages(pdf_path) as pdf:\n",
    "    plt.figure(figsize=(8.27,11.69)); plt.axis(\"off\")\n",
    "    txt = [\"PPO sobre PointEnv (discreto)\", \"\", f\"Fecha: {datetime.now().isoformat()}\"]\n",
    "    plt.text(0.02, 0.98, \"\\n\".join(txt), va=\"top\", wrap=True, fontsize=11); pdf.savefig(); plt.close()\n",
    "    plt.figure(figsize=(8.27,11.69)); plt.axis(\"off\")\n",
    "    code = [\"Fragmentos: PPO clip, GAE, update_epochs\", \"\", \"Clip:\", str(CLIP)]\n",
    "    plt.text(0.02, 0.98, \"\\n\".join(code), va=\"top\", wrap=True, fontsize=10); pdf.savefig(); plt.close()\n",
    "    img = plt.imread(plot_path); plt.figure(figsize=(8.27,11.69)); plt.imshow(img); plt.axis(\"off\"); pdf.savefig(); plt.close()\n",
    "\n",
    "readme_path = os.path.join(OUT_DIR, \"README.txt\")\n",
    "with open(readme_path, \"w\") as f:\n",
    "    f.write(\"PPO PointEnv - Output\\n\"); f.write(f\"Model: {model_path}\\nPlot: {plot_path}\\nPDF: {pdf_path}\\n\")\n",
    "print(\"PPO: terminado. Archivos en:\", OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b906334",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## 3) SAC — *Soft Actor–Critic* (continuo)\n",
    "\n",
    "**Política estocástica gaussiana con Tanh** para accionar en \\([-1, 1]\\),  \n",
    "**dos críticos (Q1, Q2)**, *target networks* y **\\(\\alpha\\)** automático para regular la entropía.\n",
    "Incluye *replay buffer* y *soft updates* (\\(\\tau\\)).\n",
    "\n",
    "**Ejecución:** al correr la celda siguiente, entrena por 400 episodios y genera:\n",
    "- `./output_sac_pointenv/sac_pointenv_models.pth` (actor + Qs)\n",
    "- `./output_sac_pointenv/convergence_sac.png`\n",
    "- `./output_sac_pointenv/SAC_PointEnv_Report.pdf`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b83770",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "sac_pointenv.py\n",
    "Soft Actor-Critic en entorno continuo 1D (PointEnvContinuous).\n",
    "Guarda: modelo (actor + critics), convergence.png (reward por episodio),\n",
    "SAC_PointEnv_Report.pdf, README.txt\n",
    "Ejecutar: python sac_pointenv.py\n",
    "Requerimientos: torch, numpy, matplotlib\n",
    "\"\"\"\n",
    "import os, random, math\n",
    "import numpy as np\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from datetime import datetime\n",
    "from collections import deque\n",
    "\n",
    "OUT_DIR = os.environ.get(\"OUT_SAC\", \"./output_sac_pointenv\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# ----------------------------\n",
    "# Entorno continuo\n",
    "class PointEnvContinuous:\n",
    "    def __init__(self, max_steps=50, force=0.6, noise_scale=0.02):\n",
    "        self.max_steps = max_steps\n",
    "        self.force = force\n",
    "        self.noise_scale = noise_scale\n",
    "        self.reset()\n",
    "    def reset(self):\n",
    "        self.pos = np.random.uniform(-1.5, 1.5)\n",
    "        self.steps = 0\n",
    "        self.target = 0.0\n",
    "        return np.array([self.pos], dtype=np.float32)\n",
    "    def step(self, action):\n",
    "        # action in [-1,1] scalar\n",
    "        act = float(np.clip(action, -1.0, 1.0))\n",
    "        noise = np.random.normal(scale=self.noise_scale)\n",
    "        self.pos = self.pos + act * self.force + noise\n",
    "        self.steps += 1\n",
    "        dist = abs(self.pos - self.target)\n",
    "        reward = -dist\n",
    "        done = False\n",
    "        if dist < 0.05:\n",
    "            reward += 1.0\n",
    "            done = True\n",
    "        if self.steps >= self.max_steps:\n",
    "            done = True\n",
    "        return np.array([self.pos], dtype=np.float32), float(reward), done, {}\n",
    "    @property\n",
    "    def obs_shape(self):\n",
    "        return (1,)\n",
    "    @property\n",
    "    def action_dim(self):\n",
    "        return 1\n",
    "\n",
    "# ----------------------------\n",
    "# Replay buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, maxlen=100000):\n",
    "        self.maxlen = maxlen\n",
    "        self.buf = deque(maxlen=maxlen)\n",
    "    def push(self, s, a, r, s2, d):\n",
    "        self.buf.append((s, a, r, s2, d))\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buf, batch_size)\n",
    "        s, a, r, s2, d = zip(*batch)\n",
    "        return (np.vstack(s), np.vstack(a), np.array(r, dtype=np.float32),\n",
    "                np.vstack(s2), np.array(d, dtype=np.float32))\n",
    "    def __len__(self):\n",
    "        return len(self.buf)\n",
    "\n",
    "# ----------------------------\n",
    "# Networks\n",
    "LOG_STD_MIN = -20\n",
    "LOG_STD_MAX = 2\n",
    "\n",
    "class GaussianActor(nn.Module):\n",
    "    def __init__(self, obs_dim, action_dim, hidden=64):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(nn.Linear(obs_dim, hidden), nn.ReLU(), nn.Linear(hidden, hidden), nn.ReLU())\n",
    "        self.mean = nn.Linear(hidden, action_dim)\n",
    "        self.log_std = nn.Linear(hidden, action_dim)\n",
    "    def forward(self, x):\n",
    "        h = self.fc(x)\n",
    "        mean = self.mean(h)\n",
    "        log_std = self.log_std(h).clamp(LOG_STD_MIN, LOG_STD_MAX)\n",
    "        std = torch.exp(log_std)\n",
    "        return mean, std\n",
    "    def sample(self, x):\n",
    "        mean, std = self.forward(x)\n",
    "        normal = torch.distributions.Normal(mean, std)\n",
    "        z = normal.rsample()\n",
    "        action = torch.tanh(z)\n",
    "        logp = normal.log_prob(z) - torch.log(1 - action.pow(2) + 1e-6)\n",
    "        logp = logp.sum(-1, keepdim=True)\n",
    "        return action, logp, torch.tanh(mean)\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, obs_dim, action_dim, hidden=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(obs_dim + action_dim, hidden), nn.ReLU(),\n",
    "                                 nn.Linear(hidden, hidden), nn.ReLU(), nn.Linear(hidden, 1))\n",
    "    def forward(self, s, a):\n",
    "        x = torch.cat([s, a], dim=-1)\n",
    "        return self.net(x).squeeze(-1)\n",
    "\n",
    "# ----------------------------\n",
    "# Hyperparams\n",
    "SEED = 42; np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "NUM_EPISODES = 400\n",
    "MAX_STEPS = 50\n",
    "GAMMA = 0.99\n",
    "LR = 3e-4\n",
    "BATCH_SIZE = 64\n",
    "REPLAY_INIT = 1000\n",
    "REPLAY_SIZE = 100000\n",
    "TAU = 0.005\n",
    "AUTO_ALPHA = True\n",
    "TARGET_ENTROPY = -1.0  # for 1-d action space\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Setup\n",
    "env = PointEnvContinuous(max_steps=MAX_STEPS)\n",
    "obs_dim = env.obs_shape[0]; action_dim = env.action_dim\n",
    "buffer = ReplayBuffer(maxlen=REPLAY_SIZE)\n",
    "\n",
    "actor = GaussianActor(obs_dim, action_dim).to(DEVICE)\n",
    "q1 = QNetwork(obs_dim, action_dim).to(DEVICE)\n",
    "q2 = QNetwork(obs_dim, action_dim).to(DEVICE)\n",
    "q1_target = QNetwork(obs_dim, action_dim).to(DEVICE); q2_target = QNetwork(obs_dim, action_dim).to(DEVICE)\n",
    "q1_target.load_state_dict(q1.state_dict()); q2_target.load_state_dict(q2.state_dict())\n",
    "\n",
    "actor_opt = optim.Adam(actor.parameters(), lr=LR)\n",
    "q1_opt = optim.Adam(q1.parameters(), lr=LR)\n",
    "q2_opt = optim.Adam(q2.parameters(), lr=LR)\n",
    "\n",
    "# alpha\n",
    "if AUTO_ALPHA:\n",
    "    log_alpha = torch.tensor(0.0, requires_grad=True, device=DEVICE)\n",
    "    alpha_opt = optim.Adam([log_alpha], lr=LR)\n",
    "else:\n",
    "    alpha = 0.2\n",
    "\n",
    "episode_rewards = []\n",
    "smoothed = []\n",
    "alpha_smooth = 0.04\n",
    "\n",
    "# Interaction and training\n",
    "total_steps = 0\n",
    "for ep in range(1, NUM_EPISODES + 1):\n",
    "    s = env.reset()\n",
    "    s_t = torch.tensor(s, dtype=torch.float32, device=DEVICE)\n",
    "    ep_reward = 0.0\n",
    "    for step in range(MAX_STEPS):\n",
    "        # sample action from current policy (for exploration)\n",
    "        with torch.no_grad():\n",
    "            action_t, _, _ = actor.sample(s_t.unsqueeze(0))\n",
    "            action = action_t.cpu().numpy().reshape(-1)\n",
    "        next_s, r, done, _ = env.step(float(action[0]))\n",
    "        buffer.push(s.reshape(1,-1), action.reshape(1,-1), r, next_s.reshape(1,-1), done)\n",
    "        s = next_s\n",
    "        s_t = torch.tensor(s, dtype=torch.float32, device=DEVICE)\n",
    "        ep_reward += r\n",
    "        total_steps += 1\n",
    "\n",
    "        # update if enough data\n",
    "        if len(buffer) > REPLAY_INIT:\n",
    "            s_b, a_b, r_b, s2_b, d_b = buffer.sample(BATCH_SIZE)\n",
    "            s_b = torch.tensor(s_b, dtype=torch.float32, device=DEVICE)\n",
    "            a_b = torch.tensor(a_b, dtype=torch.float32, device=DEVICE)\n",
    "            r_b = torch.tensor(r_b, dtype=torch.float32, device=DEVICE)\n",
    "            s2_b = torch.tensor(s2_b, dtype=torch.float32, device=DEVICE)\n",
    "            d_b = torch.tensor(d_b, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "            # target\n",
    "            with torch.no_grad():\n",
    "                a2, logp_a2, _ = actor.sample(s2_b)\n",
    "                q1_t = q1_target(s2_b, a2)\n",
    "                q2_t = q2_target(s2_b, a2)\n",
    "                qmin = torch.min(q1_t, q2_t)\n",
    "                if AUTO_ALPHA:\n",
    "                    alpha = log_alpha.exp()\n",
    "                target = r_b + GAMMA * (1 - d_b) * (qmin - alpha * logp_a2.squeeze(-1))\n",
    "\n",
    "            # Q losses\n",
    "            q1_pred = q1(s_b, a_b)\n",
    "            q2_pred = q2(s_b, a_b)\n",
    "            q1_loss = nn.MSELoss()(q1_pred, target)\n",
    "            q2_loss = nn.MSELoss()(q2_pred, target)\n",
    "            q1_opt.zero_grad(); q1_loss.backward(); q1_opt.step()\n",
    "            q2_opt.zero_grad(); q2_loss.backward(); q2_opt.step()\n",
    "\n",
    "            # actor loss\n",
    "            a_pi, logp_pi, _ = actor.sample(s_b)\n",
    "            q1_pi = q1(s_b, a_pi); q2_pi = q2(s_b, a_pi)\n",
    "            q_pi = torch.min(q1_pi, q2_pi)\n",
    "            actor_loss = (alpha * logp_pi.squeeze(-1) - q_pi).mean()\n",
    "            actor_opt.zero_grad(); actor_loss.backward(); actor_opt.step()\n",
    "\n",
    "            # alpha loss\n",
    "            if AUTO_ALPHA:\n",
    "                alpha_loss = -(log_alpha * (logp_pi + TARGET_ENTROPY).detach()).mean()\n",
    "                alpha_opt.zero_grad(); alpha_loss.backward(); alpha_opt.step()\n",
    "                alpha = log_alpha.exp().item()\n",
    "\n",
    "            # soft updates\n",
    "            for param, target_param in zip(q1.parameters(), q1_target.parameters()):\n",
    "                target_param.data.copy_(TAU * param.data + (1 - TAU) * target_param.data)\n",
    "            for param, target_param in zip(q2.parameters(), q2_target.parameters()):\n",
    "                target_param.data.copy_(TAU * param.data + (1 - TAU) * target_param.data)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    episode_rewards.append(ep_reward)\n",
    "    if len(smoothed) == 0:\n",
    "        smoothed.append(ep_reward)\n",
    "    else:\n",
    "        smoothed.append(smoothed[-1] * (1 - alpha_smooth) + ep_reward * alpha_smooth)\n",
    "\n",
    "    if ep % 25 == 0 or ep == 1:\n",
    "        print(f\"Ep {ep}/{NUM_EPISODES} Reward {ep_reward:.3f} Avg100 {np.mean(episode_rewards[-100:]):.3f}\")\n",
    "\n",
    "# Save actor+critics\n",
    "torch.save({\n",
    "    'actor': actor.state_dict(),\n",
    "    'q1': q1.state_dict(),\n",
    "    'q2': q2.state_dict()\n",
    "}, os.path.join(OUT_DIR, \"sac_pointenv_models.pth\"))\n",
    "\n",
    "# Plot\n",
    "episodes = np.arange(1, len(episode_rewards) + 1)\n",
    "plt.figure(figsize=(9,5))\n",
    "plt.plot(episodes, episode_rewards, alpha=0.25, label=\"Reward por episodio\")\n",
    "plt.plot(episodes, smoothed, label=\"Media exponencial\")\n",
    "plt.xlabel(\"Episodio\"); plt.ylabel(\"Reward\"); plt.title(\"SAC Convergencia - PointEnv Continuous\")\n",
    "plt.legend(); plt.grid(True)\n",
    "plot_path = os.path.join(OUT_DIR, \"convergence_sac.png\")\n",
    "plt.savefig(plot_path, bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "# PDF\n",
    "pdf_path = os.path.join(OUT_DIR, \"SAC_PointEnv_Report.pdf\")\n",
    "with PdfPages(pdf_path) as pdf:\n",
    "    plt.figure(figsize=(8.27,11.69)); plt.axis(\"off\")\n",
    "    txt = [\"SAC aplicado a PointEnv (continuo)\", \"\", f\"Fecha: {datetime.now().isoformat()}\"]\n",
    "    plt.text(0.02, 0.98, \"\\n\".join(txt), va=\"top\", wrap=True, fontsize=11); pdf.savefig(); plt.close()\n",
    "    plt.figure(figsize=(8.27,11.69)); plt.axis(\"off\")\n",
    "    code = [\"Fragmentos: actor gaussiano (tanh), 2 Qs, auto-alpha, replay buffer\", \"\", f\"REPLAY_INIT: {REPLAY_INIT}\"]\n",
    "    plt.text(0.02, 0.98, \"\\n\".join(code), va=\"top\", wrap=True, fontsize=10); pdf.savefig(); plt.close()\n",
    "    img = plt.imread(plot_path); plt.figure(figsize=(8.27,11.69)); plt.imshow(img); plt.axis(\"off\"); pdf.savefig(); plt.close()\n",
    "\n",
    "# README\n",
    "readme_path = os.path.join(OUT_DIR, \"README.txt\")\n",
    "with open(readme_path, \"w\") as f:\n",
    "    f.write(\"SAC PointEnv - Output\\n\"); f.write(f\"Model bundle: {os.path.join(OUT_DIR,'sac_pointenv_models.pth')}\\n\")\n",
    "    f.write(f\"Plot: {plot_path}\\nPDF: {pdf_path}\\nEpisodes: {NUM_EPISODES}\\n\")\n",
    "print(\"SAC: terminado. Archivos en:\", OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b83d07",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "### Notas\n",
    "- Los tres bloques reproducen **exactamente** tu código fuente original, sin cambios en la lógica.\n",
    "- Podés ajustar hiperparámetros dentro de cada celda (número de episodios, LR, etc.).\n",
    "- Si querés correrlos por separado, podés reiniciar el kernel entre corridas para limpiar memoria/GPU.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
